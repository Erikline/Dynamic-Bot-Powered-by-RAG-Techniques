import sys
__import__('pysqlite3')
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import os
import json
import re
import tempfile
import streamlit as st
from PIL import Image
import logging
import warnings
from dotenv import load_dotenv
import torch
import shutil
import hashlib


from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain_chroma import Chroma 
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI

load_dotenv()

# --- é…ç½® ---
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
SILICONFLOW_API_BASE = "https://api.siliconflow.cn/v1"

# --- æ¨¡å‹é…ç½® ---
MODEL_ID = "THUDM/GLM-4-9B-0414"

# --- æŒä¹…åŒ–é…ç½® ---
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç»å¯¹è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PERSIST_DIRECTORY = os.path.join(BASE_DIR, "data", "chroma_db")
PERSIST_STATE_FILE = os.path.join(BASE_DIR, "data", "processed_files_state.json")

# --- åº”ç”¨è®¾ç½® ---
APP_TITLE = f"ä½¿ç”¨ Deepseek-R1 å’Œ åŸºäº BGE è¯åµŒå…¥ çš„ ChromaDB çš„ RAG åº”ç”¨ (æ”¯æŒæŒä¹…åŒ–ä¸æ–‡ä»¶åŒæ­¥)"
FAVICON_PATH = "Bot.png"
LOGO_PATH = "icon.png"

# --- Streamlit é¡µé¢é…ç½® ---
try:
    if os.path.exists(FAVICON_PATH):
        favicon = Image.open(FAVICON_PATH)
        st.set_page_config(page_title=APP_TITLE, page_icon=favicon, layout="wide")
    else:
        st.set_page_config(page_title=APP_TITLE, layout="wide")
        # ä»…åœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶è­¦å‘Šï¼Œé¿å…åˆ·æ–°æ—¶ä¸€ç›´å¼¹çª—
        # st.warning(f"åœ¨ç›¸å¯¹è·¯å¾„ '{FAVICON_PATH}' æœªæ‰¾åˆ° Favicon å›¾æ ‡ã€‚")
except Exception as e:
    st.set_page_config(page_title=APP_TITLE, layout="wide")
    logging.warning(f"æ— æ³•åŠ è½½ Favicon: {e}")

# --- Streamlit ä¾§è¾¹æ  ---
try:
    if os.path.exists(LOGO_PATH):
        st.sidebar.image(LOGO_PATH, use_container_width=True)
    else:
        # st.sidebar.warning(f"æœªæ‰¾åˆ° Logo å›¾ç‰‡: {LOGO_PATH}")
        pass
except Exception as e:
     st.sidebar.warning(f"æ— æ³•åŠ è½½ Logo å›¾ç‰‡: {e}")

with st.sidebar:
    st.markdown(f"**{APP_TITLE}**")
    
    st.divider()

    # æ·»åŠ ä¸€ä¸ªæŒ‰é’®ç”¨äºæ¸…é™¤ç¼“å­˜å’ŒæŒä¹…åŒ–æ•°æ®
    if st.button("æ¸…é™¤æ‰€æœ‰æ•°æ®ç¼“å­˜ (åŒ…æ‹¬ ChromaDB)", key="clear_cache_and_db"):
         st.cache_resource.clear() # æ¸…é™¤ Streamlit çš„èµ„æºç¼“å­˜
         if os.path.exists(PERSIST_DIRECTORY) or os.path.exists(PERSIST_STATE_FILE):
             st.info(f"æ­£åœ¨æ¸…ç†æ—§çš„æ•°æ®ç›®å½•...")
             try:
                 if os.path.exists(PERSIST_DIRECTORY):
                    shutil.rmtree(PERSIST_DIRECTORY) 
                 
                 # åŒæ—¶ä¹Ÿåˆ é™¤çŠ¶æ€è®°å½•æ–‡ä»¶ï¼Œç¡®ä¿é€»è¾‘é‡ç½®
                 if os.path.exists(PERSIST_STATE_FILE):
                     os.remove(PERSIST_STATE_FILE)

                 st.sidebar.success("å·²æ¸…é™¤ç¼“å­˜å’ŒæŒä¹…åŒ–æ•°æ®ã€‚è¯·åˆ·æ–°é¡µé¢ã€‚")
                 st.rerun() 
             except Exception as e:
                 st.sidebar.error(f"æ¸…é™¤ç›®å½•å¤±è´¥: {e}")
                 logging.error(f"æ¸…é™¤å¤±è´¥: {e}", exc_info=True)
         else:
             st.sidebar.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…é™¤çš„æŒä¹…åŒ–æ•°æ®ã€‚")

# --- æ–‡ä»¶å¤„ç†è¾…åŠ©å‡½æ•° ---
def calculate_file_hash(uploaded_file):
    """è®¡ç®—ä¸Šä¼ æ–‡ä»¶çš„ SHA256 å“ˆå¸Œå€¼ã€‚"""
    sha256 = hashlib.sha256()
    uploaded_file.seek(0)
    while True:
        data = uploaded_file.read(8192)
        if not data:
            break
        sha256.update(data)
    uploaded_file.seek(0)
    return sha256.hexdigest()

def load_processed_state():
    """ä»çŠ¶æ€æ–‡ä»¶åŠ è½½ä¹‹å‰å¤„ç†çš„æ–‡ä»¶çŠ¶æ€ï¼ˆæ–‡ä»¶åå’Œå“ˆå¸Œå€¼ï¼‰ã€‚"""
    if os.path.exists(PERSIST_STATE_FILE):
        try:
            with open(PERSIST_STATE_FILE, 'r') as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"åŠ è½½æ–‡ä»¶çŠ¶æ€æ–‡ä»¶å¤±è´¥ ({PERSIST_STATE_FILE}): {e}")
            return {}
    return {}

def save_processed_state(state_dict):
    """å°†å½“å‰æ–‡ä»¶çŠ¶æ€ï¼ˆæ–‡ä»¶åå’Œå“ˆå¸Œå€¼ï¼‰ä¿å­˜åˆ°çŠ¶æ€æ–‡ä»¶ã€‚"""
    # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(PERSIST_STATE_FILE), exist_ok=True)
    try:
        with open(PERSIST_STATE_FILE, 'w') as f:
            json.dump(state_dict, f, indent=4)
    except Exception as e:
        logging.error(f"ä¿å­˜æ–‡ä»¶çŠ¶æ€æ–‡ä»¶å¤±è´¥ ({PERSIST_STATE_FILE}): {e}", exc_info=True)


# --- æ ¸å¿ƒ RAG åŠŸèƒ½ (configure_retriever) ---
@st.cache_resource(ttl="2h") 
def configure_retriever(uploaded_files):
    """
    åŸºäºä¸Šä¼ çš„æ–‡ä»¶æˆ–ç°æœ‰çš„æŒä¹…åŒ–æ•°æ®é…ç½®å¹¶è¿”å›ä¸€ä¸ª ChromaDB æ£€ç´¢å™¨ã€‚
    è¿”å›ä¸€ä¸ªå…ƒç»„ï¼š(æ£€ç´¢å™¨å¯¹è±¡, æ˜¯å¦ä»ç£ç›˜åŠ è½½çš„å¸ƒå°”å€¼)
    """
    temp_dir = None
    chroma_retriever = None
    was_loaded_from_disk = False

    # 1. åˆ›å»º Embedding æ¨¡å‹
    try:
        st.info("æ­£åœ¨åˆå§‹åŒ– BGE åµŒå…¥æ¨¡å‹ (ç”¨äº Chroma)...")
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        st.info(f"æ£€æµ‹åˆ°å¯ç”¨è®¾å¤‡: {device}")
        
        # æ–°ç‰ˆ langchain-huggingface å†™æ³•
        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-large-zh-v1.5",
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True}
        )
        st.success("BGE åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸã€‚")

    except Exception as e:
        st.error(f"åˆå§‹åŒ– BGE åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
        logging.error("Embedding åˆå§‹åŒ–å¤±è´¥:", exc_info=True)
        return None, False

    # 2. æ£€æŸ¥æ–‡ä»¶çŠ¶æ€å¹¶å†³å®šåŠ è½½æˆ–é‡å»º
    current_files_state = {}
    if uploaded_files:
        st.info("æ­£åœ¨è®¡ç®—ä¸Šä¼ æ–‡ä»¶çš„å“ˆå¸Œå€¼...")
        try:
            for file in uploaded_files:
                current_files_state[file.name] = calculate_file_hash(file)
        except Exception as e:
            st.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼å¤±è´¥: {e}")
            return None, False

    previous_files_state = load_processed_state()

    needs_rebuild = False
    if uploaded_files:
        if not previous_files_state:
            st.info("æœªæ‰¾åˆ°ä¹‹å‰çš„æ–‡æ¡£æ•°æ®çŠ¶æ€ï¼Œå°†åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨ã€‚")
            needs_rebuild = True
        elif current_files_state != previous_files_state:
            st.info("æ£€æµ‹åˆ°ä¸Šä¼ æ–‡ä»¶ä¸ä¹‹å‰ä¿å­˜çš„æ•°æ®çŠ¶æ€ä¸ä¸€è‡´ï¼Œå°†é‡æ–°åˆ›å»ºå‘é‡å­˜å‚¨ã€‚")
            needs_rebuild = True
        else:
            st.info("ä¸Šä¼ æ–‡ä»¶ä¸ä¹‹å‰ä¿å­˜çš„æ•°æ®çŠ¶æ€ä¸€è‡´ï¼Œå°†å°è¯•åŠ è½½ç°æœ‰çš„å‘é‡å­˜å‚¨ã€‚")
            needs_rebuild = False
    elif previous_files_state:
        st.info("æœªä¸Šä¼ æ–°æ–‡æ¡£ï¼Œå°è¯•åŠ è½½å†å²æ•°æ®...")
        needs_rebuild = False
    else:
        st.warning("æ²¡æœ‰ä¸Šä¼ æ–°çš„æ–‡æ¡£ï¼Œä¹Ÿæœªæ‰¾åˆ°ä¹‹å‰çš„æ–‡æ¡£æ•°æ®ã€‚è¯·ä¸Šä¼  PDF æ–‡æ¡£ä»¥å¼€å§‹ã€‚")
        return None, False

    # 3. æ ¹æ®åˆ¤æ–­ç»“æœæ‰§è¡ŒåŠ è½½æˆ–é‡å»º
    if needs_rebuild:
        if os.path.exists(PERSIST_DIRECTORY):
            try:
                shutil.rmtree(PERSIST_DIRECTORY)
                st.success("æ—§çš„ Chroma ç›®å½•å·²æ¸…ç†ã€‚")
            except Exception as cleanup_e:
                st.error(f"æ¸…ç†æ—§çš„ Chroma ç›®å½•å¤±è´¥: {cleanup_e}")

        all_splits = []
        try:
            temp_dir = tempfile.TemporaryDirectory()
            st.info(f"å¼€å§‹å¤„ç† {len(uploaded_files)} ä¸ªä¸Šä¼ çš„æ–‡ä»¶...")
            
            # è¿›åº¦æ¡
            progress_bar = st.progress(0)
            
            for idx, file in enumerate(uploaded_files):
                temp_filepath = os.path.join(temp_dir.name, file.name)
                try:
                    file.seek(0)
                    with open(temp_filepath, "wb") as f:
                        f.write(file.getvalue())
                    
                    loader = PyPDFLoader(temp_filepath)
                    file_docs = loader.load()
                    if not file_docs:
                         logging.warning(f"æ–‡ä»¶ {file.name} ä¸ºç©ºã€‚")
                         continue
                    
                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
                    splits = text_splitter.split_documents(file_docs)
                    
                    # ä¸ºæ–‡æ¡£æ·»åŠ å…ƒæ•°æ®æº
                    for split in splits:
                        split.metadata["source"] = file.name
                        
                    all_splits.extend(splits)
                    
                    # æ›´æ–°è¿›åº¦
                    progress_bar.progress((idx + 1) / len(uploaded_files))
                    
                except Exception as e:
                    st.error(f"å¤„ç† PDF {file.name} å‡ºé”™: {e}")

            if not all_splits:
                st.warning("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡æ¡£ã€‚")
                return None, False

            try:
                st.info(f"æ­£åœ¨åˆ›å»º Chroma å‘é‡å­˜å‚¨å¹¶æŒä¹…åŒ–åˆ° {PERSIST_DIRECTORY}...")
                
                # æ–°ç‰ˆ langchain-chroma å†™æ³•
                vectordb = Chroma.from_documents(
                    documents=all_splits,
                    embedding=embeddings,
                    persist_directory=PERSIST_DIRECTORY
                )
                
                st.success(f"Chroma å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸã€‚")

                save_processed_state(current_files_state)
                st.success(f"å·²ä¿å­˜å½“å‰æ–‡ä»¶çŠ¶æ€ã€‚")

                chroma_retriever = vectordb.as_retriever(
                    search_type="mmr", 
                    search_kwargs={"k": 6} # è¿™é‡Œæˆ‘è°ƒæ•´äº† k å€¼ï¼Œä½ å¯ä»¥æ”¹å›å»
                )
                return chroma_retriever, False

            except Exception as e:
                st.error(f"åˆ›å»º Chroma å‘é‡å­˜å‚¨å¤±è´¥: {e}")
                return None, False

        except Exception as e:
            st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return None, False
        finally:
            if temp_dir:
                try:
                    temp_dir.cleanup()
                except:
                    pass

    else: # å°è¯•ä»ç£ç›˜åŠ è½½
        if not os.path.exists(PERSIST_DIRECTORY):
             st.warning(f"æŒä¹…åŒ–ç›®å½•ä¸å­˜åœ¨ã€‚è¯·ä¸Šä¼ æ–‡ä»¶ã€‚")
             return None, False

        try:
            st.info(f"æ­£åœ¨åŠ è½½ Chroma å‘é‡å­˜å‚¨...")
            # æ–°ç‰ˆåŠ è½½æ–¹å¼
            vectordb = Chroma(
                persist_directory=PERSIST_DIRECTORY, 
                embedding_function=embeddings
            )
            
            chroma_retriever = vectordb.as_retriever(
                search_type="mmr",
                search_kwargs={"k": 6}
            )
            st.success("Chroma æ£€ç´¢å™¨åŠ è½½æˆåŠŸã€‚")
            was_loaded_from_disk = True
            return chroma_retriever, was_loaded_from_disk

        except Exception as e:
            st.error(f"åŠ è½½å¤±è´¥ ({e})ã€‚å¯èƒ½æ˜¯æ•°æ®ç‰ˆæœ¬ä¸å…¼å®¹ï¼Œè¯·ç‚¹å‡»'æ¸…é™¤æ‰€æœ‰æ•°æ®ç¼“å­˜'ã€‚")
            return None, False


# --- Streamlit åº”ç”¨ä¸»é€»è¾‘ ---

# 1. æ–‡ä»¶ä¸Šä¼ 
uploaded_files = st.sidebar.file_uploader(
    label="ä¸Šä¼  PDF æ–‡ä»¶",
    type=["pdf"],
    accept_multiple_files=True,
    key="pdf_uploader_main"
)

# 2. é…ç½®æ£€ç´¢å™¨
retriever = None 
loaded_from_disk = False 

if uploaded_files or os.path.exists(PERSIST_DIRECTORY):
    with st.spinner("æ­£åœ¨åˆå§‹åŒ–æ£€ç´¢å™¨..."):
        retriever, loaded_from_disk = configure_retriever(uploaded_files)

# 3. åˆå§‹åŒ– LLM
llm = None 
if retriever is not None:
    try:
        st.info(f"æ­£åœ¨åˆå§‹åŒ– LLM: {MODEL_ID} (SiliconFlow)...")
        
        # æ ¹æ®ä½ çš„ curl å‘½ä»¤é…ç½®å‚æ•°
        llm = ChatOpenAI(
            model_name=MODEL_ID,
            openai_api_key=SILICONFLOW_API_KEY,
            openai_api_base=SILICONFLOW_API_BASE,
            
            # æ ‡å‡†å‚æ•°
            temperature=0.7,        # curl ä¸­çš„è®¾ç½®
            max_tokens=4096,        # curl ä¸­çš„è®¾ç½®
            
            # é¢å¤–å‚æ•° (å¯¹åº” curl ä¸­çš„ top_p, top_k, frequency_penalty ç­‰)
            model_kwargs={
                "top_p": 0.7,
                # "top_k": 50,
                "frequency_penalty": 0.5,
                # "min_p": 0.05, # LangChain éƒ¨åˆ†ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒä¼ è¿™ä¸ªï¼Œå¦‚æœæŠ¥é”™è¯·æ³¨é‡Šæ‰
            }
        )
        st.success(f"LLM ({MODEL_ID}) åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        st.error(f"åˆå§‹åŒ– LLM å¤±è´¥: {e}")
        st.stop()

# 4. åˆå§‹åŒ–èŠå¤©è®°å½• (ä½¿ç”¨ LangChain çš„ Streamlit å†å²è®°å½•ç±»)
msgs = StreamlitChatMessageHistory(key="chat_messages_history")

# 5. å®šä¹‰ Prompt
RESPONSE_TEMPLATE = """<s>[INST]
<<SYS>>
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€è€å¿ƒã€ä¸”ä¹äºåŠ©äººçš„ AI åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥ä¾æ®ä¸Šä¸‹æ–‡è¿›è¡Œå›ç­”ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œæˆ–è€…é—®é¢˜ä¸ä¸Šä¸‹æ–‡æ— å…³ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ã€‚
å›ç­”åº”æ¸…æ™°ã€ç®€æ´ï¼Œå¹¶ä½¿ç”¨ä¸­æ–‡ã€‚
<<SYS>>

ä¸Šä¸‹æ–‡ä¿¡æ¯:
---
{context}
---

ç”¨æˆ·é—®é¢˜: {question}
[/INST]
AI åŠ©æ‰‹å›ç­”:
"""
PROMPT = PromptTemplate(template=RESPONSE_TEMPLATE, input_variables=["context", "question"])

# 6. åˆ›å»º RAG é—®ç­”é“¾
qa_chain = None 
if llm is not None and retriever is not None:
    try:
        # st.info("æ­£åœ¨åˆ›å»º RAG é—®ç­”é“¾...")
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type='stuff',
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
    except Exception as e:
        st.error(f"åˆ›å»º RAG é“¾å¤±è´¥: {e}")
        st.stop()

# 7. æ˜¾ç¤ºèŠå¤©ç•Œé¢

# åˆå§‹æ¬¢è¿è¯­
if len(msgs.messages) == 0:
    if retriever is not None: 
        if loaded_from_disk:
            initial_message = f"ğŸ“š å·²åŠ è½½å†å²æ–‡æ¡£åº“ã€‚è¯·é—®æœ‰ä»€ä¹ˆå…³äºè¿™äº›æ–‡æ¡£çš„é—®é¢˜å—ï¼Ÿ"
        elif uploaded_files:
            initial_message = f"ğŸ“„ æ–‡æ¡£å·²å¤„ç†å®Œæ¯•ã€‚è¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ"
        else:
            initial_message = "è¯·ä¸Šä¼ æ–‡æ¡£ã€‚"
    else:
        initial_message = "ğŸ‘‹ ä½ å¥½ï¼è¯·åœ¨å·¦ä¾§ä¸Šä¼  PDF æ–‡æ¡£ï¼Œæˆ‘å°†æ ¹æ®æ–‡æ¡£å†…å®¹å›ç­”ä½ çš„é—®é¢˜ã€‚"
        
    msgs.add_ai_message(initial_message)

# å¤´åƒæ˜ å°„
avatars = {"human": "ğŸ§‘â€ğŸ’»", "ai": "ğŸ¤–"}

# æ¸²æŸ“å†å²æ¶ˆæ¯
for msg in msgs.messages:
    st.chat_message(msg.type, avatar=avatars.get(msg.type)).write(msg.content)

# å¤„ç†ç”¨æˆ·è¾“å…¥
chat_input_disabled = (qa_chain is None)
placeholder_text = "è¯·å…ˆä¸Šä¼ æ–‡æ¡£..." if chat_input_disabled else "è¾“å…¥ä½ çš„é—®é¢˜..."

if user_query := st.chat_input(placeholder=placeholder_text, disabled=chat_input_disabled):
    msgs.add_user_message(user_query)
    st.chat_message("human", avatar=avatars["human"]).write(user_query)

    with st.chat_message("ai", avatar=avatars["ai"]):
        placeholder = st.empty()
        placeholder.markdown("â³ æ­£åœ¨æ€è€ƒ...")
        try:
            # è°ƒç”¨ QA é“¾
            response = qa_chain.invoke({"query": user_query})
            answer = response.get("result")
            source_docs = response.get("source_documents", [])

            if not answer:
                 answer = "æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆå›ç­”ã€‚"

            placeholder.markdown(answer)
            msgs.add_ai_message(answer)

            # æ˜¾ç¤ºæ¥æº
            if source_docs:
                with st.expander("ğŸ” æŸ¥çœ‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥æº", expanded=False):
                     for i, doc in enumerate(source_docs):
                        source = doc.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶')
                        # ç®€åŒ–æ–‡ä»¶åæ˜¾ç¤º
                        source_name = os.path.basename(source)
                        page = doc.metadata.get('page', '?')
                        if isinstance(page, int): page += 1
                        
                        st.markdown(f"**ğŸ“„ æ¥æº {i+1}:** `{source_name}` (ç¬¬ {page} é¡µ)")
                        content_preview = doc.page_content[:300].replace("\n", " ")
                        st.caption(f"{content_preview}...")
                        st.divider()

        except Exception as e:
            placeholder.empty()
            st.error(f"å‘ç”Ÿé”™è¯¯: {e}")
            msgs.add_ai_message(f"å‘ç”Ÿé”™è¯¯: {e}")

# --- ä¾§è¾¹æ  "å…³äº" éƒ¨åˆ† ---
with st.sidebar:
    st.divider()
    about = st.expander("å…³äºæ­¤åº”ç”¨")
    
    status_text = "æœªçŸ¥"
    if retriever:
        if loaded_from_disk: status_text = "å·²åŠ è½½å†å²æ•°æ®"
        else: status_text = "ä½¿ç”¨å½“å‰ä¸Šä¼ æ•°æ®"
    else:
        status_text = "ç­‰å¾…ä¸Šä¼ "

    about.write(f"""
    **çŠ¶æ€:** {status_text}
    
    **æŠ€æœ¯æ ˆ:**
    *   LLM: {MODEL_ID}
    *   Embedding: BGE-Large-Zh
    *   VectorDB: Chroma ({'æŒä¹…åŒ–å¼€å¯' if os.path.exists(PERSIST_DIRECTORY) else 'æ— æ•°æ®'})
    
    **åŠŸèƒ½:**
    æ–‡ä»¶å†…å®¹ä¼šè‡ªåŠ¨æŒä¹…åŒ–ä¿å­˜ã€‚ä¸‹æ¬¡æ‰“å¼€æ— éœ€é‡æ–°ä¸Šä¼ ï¼Œé™¤éæ–‡ä»¶å‘ç”Ÿå˜åŠ¨ã€‚
    """)

















