# å¯¼å…¥æ‰€éœ€çš„åº“
import os
import json
import re
import tempfile
import streamlit as st
from PIL import Image
import logging
import warnings
from dotenv import load_dotenv
import torch
import shutil
import hashlib

# åŠ è½½ç¯å¢ƒå˜é‡ (ç¡®ä¿ .env æ–‡ä»¶å­˜åœ¨æˆ–ç¯å¢ƒå˜é‡å·²è®¾ç½®)
# åœ¨éƒ¨ç½²æ—¶ï¼Œé€šå¸¸é€šè¿‡å¹³å°çš„ Secrets åŠŸèƒ½è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè€Œä¸æ˜¯ä¾èµ– .env æ–‡ä»¶
load_dotenv()

# å¿½ç•¥ç‰¹å®šçš„ Streamlit å¼ƒç”¨è­¦å‘Š
warnings.filterwarnings(
    "ignore",
    category=DeprecationWarning,
    message=".*use_column_width parameter has been deprecated.*"
)

# å¯¼å…¥ Langchain ç›¸å…³æ¨¡å—
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_huggingface import HuggingFaceEmbeddings # Embeddings ä¿æŒä¸å˜
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from langchain_core.callbacks import BaseCallbackHandler
from langchain_text_splitters import RecursiveCharacterTextSplitter
# å¯¼å…¥ DashScope (OpenAIå…¼å®¹æ¨¡å¼) æ‰€éœ€çš„ç»„ä»¶
try:
    from langchain_openai import ChatOpenAI # <--- ä½¿ç”¨ LangChain çš„ OpenAI åŒ…è£…å™¨
except ImportError:
    st.error("æ‰¾ä¸åˆ° langchain-openai åº“ã€‚è¯·å®‰è£…å®ƒï¼špip install -U langchain-openai")
    st.stop()
try:
    import openai # æ£€æŸ¥ openai åº“æœ¬èº«æ˜¯å¦å­˜åœ¨
except ImportError:
    st.error("æ‰¾ä¸åˆ° openai åº“ã€‚è¯·å®‰è£…å®ƒï¼špip install -U openai")
    st.stop()


# --- é…ç½® ---
# DashScope (ç™¾ç‚¼) API é…ç½®
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY") # ä»ç¯å¢ƒå˜é‡è·å–
DASHSCOPE_API_BASE = "https://dashscope.aliyuncs.com/compatible-mode/v1"

if not DASHSCOPE_API_KEY:
    st.error("æœªæ‰¾åˆ° DashScope API å¯†é’¥ã€‚è¯·åœ¨éƒ¨ç½²å¹³å°çš„ Secrets ä¸­è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEYã€‚")
    st.info("å¦‚ä½•è·å–API Key: https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key")
    st.stop()

# --- æ¨¡å‹é…ç½® ---
MODEL_ID = "deepseek-r1" # ä½¿ç”¨ DashScope çš„æ¨¡å‹åç§°

# --- æŒä¹…åŒ–é…ç½® ---
# ä½¿ç”¨ç›¸å¯¹äºè„šæœ¬çš„ç›¸å¯¹è·¯å¾„ï¼Œæ•°æ®å°†ä¿å­˜åœ¨é¡¹ç›®ä¸‹çš„ data/chroma_db_streamlit ç›®å½•ä¸­
# é‡è¦ï¼šç¡®ä¿å°† 'data/' ç›®å½•æ·»åŠ åˆ°ä½ çš„ .gitignore æ–‡ä»¶ä¸­ï¼
PERSIST_DIRECTORY = "data/chroma_db_streamlit"
PERSIST_STATE_FILE = os.path.join(PERSIST_DIRECTORY, "processed_files_state.json") # os.path.join ä¼šæ­£ç¡®å¤„ç†è·¯å¾„

# --- åº”ç”¨è®¾ç½® ---
APP_TITLE = f"ä½¿ç”¨ Deepseek-R1 å’Œ åŸºäº BGE è¯åµŒå…¥ çš„ ChromaDB çš„ RAG åº”ç”¨ (æ”¯æŒæŒä¹…åŒ–ä¸æ–‡ä»¶åŒæ­¥)"
# --- ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨ç›¸å¯¹äºè„šæœ¬çš„ç›¸å¯¹è·¯å¾„ (å› ä¸ºå›¾ç‰‡åœ¨æ ¹ç›®å½•) ---
FAVICON_PATH = "Bot.png"
LOGO_PATH = "icon.png"
# --- ä¿®æ”¹ç»“æŸ ---

# --- Streamlit é¡µé¢é…ç½® ---
try:
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
    if os.path.exists(FAVICON_PATH):
        favicon = Image.open(FAVICON_PATH)
        st.set_page_config(page_title=APP_TITLE, page_icon=favicon, layout="wide")
    else:
        st.set_page_config(page_title=APP_TITLE, layout="wide")
        # æ›´æ–°è­¦å‘Šä¿¡æ¯ä¸­çš„è·¯å¾„
        st.warning(f"åœ¨ç›¸å¯¹è·¯å¾„ '{FAVICON_PATH}' æœªæ‰¾åˆ° Favicon å›¾æ ‡ã€‚è¯·ç¡®ä¿æ–‡ä»¶ Bot.png ä¸ Bot.py åœ¨åŒä¸€ç›®å½•ã€‚")
except Exception as e:
    st.set_page_config(page_title=APP_TITLE, layout="wide")
    st.warning(f"æ— æ³•åŠ è½½ Favicon å›¾æ ‡ ({FAVICON_PATH}): {e}")


# --- Streamlit ä¾§è¾¹æ  ---
try:
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œä½¿ç”¨ç›¸å¯¹è·¯å¾„
    if os.path.exists(LOGO_PATH):
        st.sidebar.image(LOGO_PATH, use_container_width=True)
    else:
        # æ›´æ–°è­¦å‘Šä¿¡æ¯ä¸­çš„è·¯å¾„
        st.sidebar.warning(f"åœ¨ç›¸å¯¹è·¯å¾„ '{LOGO_PATH}' æœªæ‰¾åˆ° Logo å›¾ç‰‡ã€‚è¯·ç¡®ä¿æ–‡ä»¶ icon.png ä¸ Bot.py åœ¨åŒä¸€ç›®å½•ã€‚")
except Exception as e:
     st.sidebar.warning(f"æ— æ³•åŠ è½½ Logo å›¾ç‰‡ ({LOGO_PATH}): {e}")

with st.sidebar:
    st.markdown(f"**{APP_TITLE}**")
    # æ·»åŠ ä¸€ä¸ªæŒ‰é’®ç”¨äºæ¸…é™¤ç¼“å­˜å’ŒæŒä¹…åŒ–æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•æˆ–å¤„ç†ä¸åŒæ–‡ä»¶ï¼‰
    if st.button("æ¸…é™¤æ‰€æœ‰æ•°æ®ç¼“å­˜ (åŒ…æ‹¬ ChromaDB)", key="clear_cache_and_db"):
         st.cache_resource.clear() # æ¸…é™¤ Streamlit çš„èµ„æºç¼“å­˜
         if os.path.exists(PERSIST_DIRECTORY):
             st.info(f"æ­£åœ¨æ¸…ç†æ—§çš„ Chroma ç›®å½•: {PERSIST_DIRECTORY}")
             try:
                 shutil.rmtree(PERSIST_DIRECTORY) # æ¸…é™¤æŒä¹…åŒ–ç›®å½•
                 st.sidebar.success("å·²æ¸…é™¤ç¼“å­˜å’Œ ChromaDB æŒä¹…åŒ–æ•°æ®ã€‚è¯·åˆ·æ–°é¡µé¢å¹¶é‡æ–°ä¸Šä¼ æ–‡ä»¶ã€‚")
                 st.rerun() # é‡æ–°è¿è¡Œåº”ç”¨
             except Exception as e:
                 st.sidebar.error(f"æ¸…é™¤ ChromaDB ç›®å½•å¤±è´¥: {e}")
                 logging.error(f"æ¸…é™¤ ChromaDB ç›®å½•å¤±è´¥: {e}", exc_info=True)
         else:
             st.sidebar.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…é™¤çš„ ChromaDB æŒä¹…åŒ–æ•°æ®ã€‚")


# --- æ–‡ä»¶å¤„ç†è¾…åŠ©å‡½æ•° ---
def calculate_file_hash(uploaded_file):
    """è®¡ç®—ä¸Šä¼ æ–‡ä»¶çš„ SHA256 å“ˆå¸Œå€¼ã€‚"""
    sha256 = hashlib.sha256()
    uploaded_file.seek(0)
    while True:
        data = uploaded_file.read(8192)
        if not data:
            break
        sha256.update(data)
    uploaded_file.seek(0)
    return sha256.hexdigest()

def load_processed_state():
    """ä»çŠ¶æ€æ–‡ä»¶åŠ è½½ä¹‹å‰å¤„ç†çš„æ–‡ä»¶çŠ¶æ€ï¼ˆæ–‡ä»¶åå’Œå“ˆå¸Œå€¼ï¼‰ã€‚"""
    if os.path.exists(PERSIST_STATE_FILE):
        try:
            with open(PERSIST_STATE_FILE, 'r') as f:
                return json.load(f)
        except Exception as e:
            logging.warning(f"åŠ è½½æ–‡ä»¶çŠ¶æ€æ–‡ä»¶å¤±è´¥ ({PERSIST_STATE_FILE}): {e}")
            return {}
    return {}

def save_processed_state(state_dict):
    """å°†å½“å‰æ–‡ä»¶çŠ¶æ€ï¼ˆæ–‡ä»¶åå’Œå“ˆå¸Œå€¼ï¼‰ä¿å­˜åˆ°çŠ¶æ€æ–‡ä»¶ã€‚"""
    # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
    os.makedirs(PERSIST_DIRECTORY, exist_ok=True)
    try:
        with open(PERSIST_STATE_FILE, 'w') as f:
            json.dump(state_dict, f, indent=4)
    except Exception as e:
        logging.error(f"ä¿å­˜æ–‡ä»¶çŠ¶æ€æ–‡ä»¶å¤±è´¥ ({PERSIST_STATE_FILE}): {e}", exc_info=True)


# --- æ ¸å¿ƒ RAG åŠŸèƒ½ (configure_retriever - åº”ç”¨ GPU ä¿®æ­£ & æŒä¹…åŒ– & æ–‡ä»¶åŒæ­¥) ---
@st.cache_resource(ttl="2h") # ç¼“å­˜2å°æ—¶
def configure_retriever(uploaded_files):
    """
    åŸºäºä¸Šä¼ çš„æ–‡ä»¶æˆ–ç°æœ‰çš„æŒä¹…åŒ–æ•°æ®é…ç½®å¹¶è¿”å›ä¸€ä¸ª ChromaDB æ£€ç´¢å™¨ã€‚
    è¿”å›ä¸€ä¸ªå…ƒç»„ï¼š(æ£€ç´¢å™¨å¯¹è±¡, æ˜¯å¦ä»ç£ç›˜åŠ è½½çš„å¸ƒå°”å€¼)
    å¤„ç†æ–‡ä»¶å˜æ›´é€»è¾‘ã€‚
    """
    temp_dir = None
    chroma_retriever = None
    was_loaded_from_disk = False # æ–°å¢æ ‡å¿—

    # 1. åˆ›å»º Embedding æ¨¡å‹ (åŠ è½½å’Œåˆ›å»ºéƒ½éœ€è¦)
    try:
        st.info("æ­£åœ¨åˆå§‹åŒ– BGE åµŒå…¥æ¨¡å‹ (ç”¨äº Chroma)...")
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        st.info(f"æ£€æµ‹åˆ°å¯ç”¨è®¾å¤‡: {device}ï¼Œå°†ç”¨äºåµŒå…¥æ¨¡å‹åŠ è½½ã€‚")
        if device == 'cuda':
             try:
                st.info(f"GPU å‹å·: {torch.cuda.get_device_name(0)}")
             except Exception:
                st.warning("æ— æ³•è·å– GPU å‹å·ä¿¡æ¯ã€‚")

        model_kwargs = {'device': device}
        encode_kwargs = {'normalize_embeddings': True}

        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-large-zh-v1.5",
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )
        st.success("BGE åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸã€‚")

    except Exception as e:
        st.error(f"åˆå§‹åŒ– BGE åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
        logging.error("Embedding åˆå§‹åŒ–å¤±è´¥:", exc_info=True)
        if "out of memory" in str(e).lower():
            st.error("GPU æ˜¾å­˜ä¸è¶³ï¼è¯·å°è¯•å…³é—­å…¶ä»–å ç”¨æ˜¾å­˜çš„ç¨‹åºï¼Œæˆ–åœ¨ä»£ç ä¸­å°† device æ”¹ä¸º 'cpu'ã€‚")
        elif "meta tensor" in str(e):
            st.error("åŠ è½½æ¨¡å‹æ—¶é‡åˆ° Meta Tensor é”™è¯¯ã€‚è¯·å°è¯•æ¸…ç† Hugging Face æ¨¡å‹ç¼“å­˜ï¼Œç„¶åé‡å¯åº”ç”¨ã€‚")
        else:
             st.error("è¯·æ£€æŸ¥æ¨¡å‹åç§°ã€ç½‘ç»œè¿æ¥æˆ–ç³»ç»Ÿèµ„æºã€‚")
        return None, False

    # 2. æ£€æŸ¥æ–‡ä»¶çŠ¶æ€å¹¶å†³å®šåŠ è½½æˆ–é‡å»º
    current_files_state = {}
    if uploaded_files:
        st.info("æ­£åœ¨è®¡ç®—ä¸Šä¼ æ–‡ä»¶çš„å“ˆå¸Œå€¼ä»¥æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´...")
        try:
            for file in uploaded_files:
                current_files_state[file.name] = calculate_file_hash(file)
            st.success(f"å·²è®¡ç®— {len(uploaded_files)} ä¸ªæ–‡ä»¶çš„å“ˆå¸Œå€¼ã€‚")
        except Exception as e:
            st.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼å¤±è´¥: {e}")
            logging.error("è®¡ç®—å“ˆå¸Œå¤±è´¥:", exc_info=True)
            st.warning("æ–‡ä»¶å“ˆå¸Œè®¡ç®—å¤±è´¥ï¼Œä¸ºç¡®ä¿æ•°æ®å‡†ç¡®æ€§ï¼Œå°†å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶ã€‚")
            current_files_state = {}
            uploaded_files = []

    previous_files_state = load_processed_state()

    needs_rebuild = False
    if uploaded_files:
        if not previous_files_state:
            st.info("æœªæ‰¾åˆ°ä¹‹å‰çš„æ–‡æ¡£æ•°æ®çŠ¶æ€ï¼Œå°†æ ¹æ®å½“å‰ä¸Šä¼ æ–‡ä»¶åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨ã€‚")
            needs_rebuild = True
        elif current_files_state != previous_files_state:
            st.info("æ£€æµ‹åˆ°ä¸Šä¼ æ–‡ä»¶ä¸ä¹‹å‰ä¿å­˜çš„æ•°æ®çŠ¶æ€ä¸ä¸€è‡´ (æ–‡ä»¶æ•°é‡ã€åç§°æˆ–å†…å®¹æœ‰å˜æ›´)ï¼Œå°†é‡æ–°åˆ›å»ºå‘é‡å­˜å‚¨ã€‚")
            changed_files = [name for name, hash_val in current_files_state.items() if name not in previous_files_state or previous_files_state.get(name) != hash_val]
            removed_files_from_upload = [name for name in previous_files_state if name not in current_files_state]
            if changed_files: st.info(f"å˜åŠ¨æˆ–æ–°å¢æ–‡ä»¶: {', '.join(changed_files)}")
            if removed_files_from_upload: st.info(f"æœ¬æ¬¡æœªä¸Šä¼ ä½†ä¸Šæ¬¡å¤„ç†è¿‡çš„æ–‡ä»¶: {', '.join(removed_files_from_upload)}")
            needs_rebuild = True
        else:
            st.info("ä¸Šä¼ æ–‡ä»¶ä¸ä¹‹å‰ä¿å­˜çš„æ•°æ®çŠ¶æ€ä¸€è‡´ï¼Œå°†å°è¯•åŠ è½½ç°æœ‰çš„å‘é‡å­˜å‚¨ã€‚")
            needs_rebuild = False
    elif previous_files_state:
        st.info("æœªä¸Šä¼ æ–°çš„æ–‡æ¡£ï¼Œä½†æ£€æµ‹åˆ°ä¹‹å‰å·²å¤„ç†çš„æ–‡æ¡£æ•°æ®ã€‚æ­£åœ¨å°è¯•åŠ è½½...")
        needs_rebuild = False
    else:
        st.warning("æ²¡æœ‰ä¸Šä¼ æ–°çš„æ–‡æ¡£ï¼Œä¹Ÿæœªæ‰¾åˆ°ä¹‹å‰çš„æ–‡æ¡£æ•°æ®ã€‚è¯·ä¸Šä¼  PDF æ–‡æ¡£ä»¥å¼€å§‹ã€‚")
        return None, False

    # 3. æ ¹æ®åˆ¤æ–­ç»“æœæ‰§è¡ŒåŠ è½½æˆ–é‡å»º
    if needs_rebuild:
        if os.path.exists(PERSIST_DIRECTORY):
            st.info(f"æ­£åœ¨æ¸…ç†æ—§çš„ Chroma ç›®å½•: {PERSIST_DIRECTORY}")
            try:
                shutil.rmtree(PERSIST_DIRECTORY)
                st.success("æ—§çš„ Chroma ç›®å½•å·²æ¸…ç†ã€‚")
            except Exception as cleanup_e:
                st.error(f"æ¸…ç†æ—§çš„ Chroma ç›®å½•å¤±è´¥: {cleanup_e}")
                logging.error(f"æ¸…ç†æ—§çš„ Chroma ç›®å½•å¤±è´¥: {cleanup_e}", exc_info=True)

        docs = []
        temp_dir = None
        all_splits = []

        try:
            temp_dir = tempfile.TemporaryDirectory()
            st.info(f"å¼€å§‹å¤„ç† {len(uploaded_files)} ä¸ªä¸Šä¼ çš„æ–‡ä»¶ (ç”¨äº Chroma)...")

            for file in uploaded_files:
                temp_filepath = os.path.join(temp_dir.name, file.name)
                try:
                    file.seek(0)
                    with open(temp_filepath, "wb") as f:
                        f.write(file.getvalue())
                    loader = PyPDFLoader(temp_filepath)
                    file_docs = loader.load()
                    if not file_docs:
                         logging.warning(f"æ–‡ä»¶ {file.name} åŠ è½½åæœªäº§ç”Ÿæ–‡æ¡£ã€‚")
                         continue
                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)
                    splits = text_splitter.split_documents(file_docs)
                    all_splits.extend(splits)
                    logging.info(f"æˆåŠŸåŠ è½½å¹¶åˆ†å‰²äº† {file.name}ï¼Œäº§ç”Ÿ {len(splits)} ä¸ªç‰‡æ®µã€‚")
                except Exception as e:
                    st.error(f"åŠ è½½æˆ–å¤„ç† PDF æ–‡ä»¶ {file.name} æ—¶å‡ºé”™: {e}")
                    logging.error(f"å¤„ç† {file.name} æ—¶å‡ºé”™: {e}", exc_info=True)

            if not all_splits:
                st.warning("æ²¡æœ‰æˆåŠŸåŠ è½½æˆ–åˆ†å‰²ä»»ä½•ä¸Šä¼ çš„æ–‡æ¡£ç”¨äº Chromaã€‚")
                return None, False

            try:
                st.info(f"æ­£åœ¨åˆ›å»º Chroma å‘é‡å­˜å‚¨å¹¶æŒä¹…åŒ–åˆ° {PERSIST_DIRECTORY} (å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
                vectordb = Chroma.from_documents(
                    all_splits,
                    embeddings,
                    persist_directory=PERSIST_DIRECTORY
                )
                st.success(f"æ–°çš„ Chroma å‘é‡å­˜å‚¨åˆ›å»ºæˆåŠŸå¹¶å·²æŒä¹…åŒ–åˆ° {PERSIST_DIRECTORY}ã€‚")

                save_processed_state(current_files_state)
                st.success(f"å·²ä¿å­˜å½“å‰æ–‡ä»¶çŠ¶æ€åˆ° {PERSIST_STATE_FILE}ã€‚")

                chroma_retriever = vectordb.as_retriever(
                    search_type="mmr", # ä½¿ç”¨ MMR ä»¥å¢åŠ å¤šæ ·æ€§
                    search_kwargs={"k": 16}
                )
                st.success("Chroma æ£€ç´¢å™¨é…ç½®æˆåŠŸ (MMR, k=16)ã€‚")
                return chroma_retriever, False

            except Exception as e:
                st.error(f"åˆ›å»º Chroma å‘é‡å­˜å‚¨æˆ–æ£€ç´¢å™¨å¤±è´¥: {e}")
                logging.error("Chroma åˆ›å»ºå¤±è´¥:", exc_info=True)
                if "out of memory" in str(e).lower():
                     st.error("GPU æ˜¾å­˜ä¸è¶³ï¼")
                elif "meta tensor" in str(e):
                     st.error("åŠ è½½æ¨¡å‹æ—¶é‡åˆ° Meta Tensor é”™è¯¯ã€‚")
                return None, False

        except Exception as e:
            st.error(f"åœ¨æ£€ç´¢å™¨é…ç½®è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            logging.error("æ£€ç´¢å™¨é…ç½®å¤±è´¥:", exc_info=True)
            return None, False
        finally:
            if temp_dir:
                try:
                    temp_dir.cleanup()
                    logging.info("ä¸´æ—¶ç›®å½•å·²æ¸…ç†ã€‚")
                except Exception as e:
                    logging.warning(f"æ— æ³•æ¸…ç†ä¸´æ—¶ç›®å½•: {e}")

    else: # ä¸éœ€è¦é‡å»ºï¼Œå°è¯•ä»æŒä¹…åŒ–ç›®å½•åŠ è½½
        if not os.path.exists(PERSIST_DIRECTORY):
             st.warning(f"æŒä¹…åŒ–ç›®å½• {PERSIST_DIRECTORY} ä¸å­˜åœ¨ã€‚è¯·ä¸Šä¼ æ–‡ä»¶ä»¥åˆ›å»ºæ•°æ®åº“ã€‚")
             return None, False # ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•åŠ è½½

        try:
            st.info(f"æ­£åœ¨å°è¯•ä»æŒä¹…åŒ–ç›®å½•åŠ è½½ Chroma å‘é‡å­˜å‚¨: {PERSIST_DIRECTORY}...")
            vectordb = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)
            st.success("æˆåŠŸåŠ è½½ç°æœ‰çš„ Chroma å‘é‡å­˜å‚¨ã€‚")
            chroma_retriever = vectordb.as_retriever(
                search_type="mmr",
                search_kwargs={"k": 16}
            )
            st.success("Chroma æ£€ç´¢å™¨åŠ è½½æˆåŠŸ (MMR, k=16)ã€‚")
            was_loaded_from_disk = True
            return chroma_retriever, was_loaded_from_disk

        except Exception as e:
            st.error(f"ä»æŒä¹…åŒ–ç›®å½• {PERSIST_DIRECTORY} åŠ è½½ Chroma å¤±è´¥ ({e})ã€‚")
            logging.warning(f"Chroma åŠ è½½å¤±è´¥: {e}", exc_info=True)
            st.warning("åŠ è½½ç°æœ‰æ•°æ®å¤±è´¥ï¼Œå¯èƒ½æ˜¯æ•°æ®æŸåæˆ–ç‰ˆæœ¬ä¸å…¼å®¹ã€‚è¯·å°è¯•é‡æ–°ä¸Šä¼ æ–‡ä»¶æˆ–ç‚¹å‡»ä¾§è¾¹æ 'æ¸…é™¤æ‰€æœ‰æ•°æ®ç¼“å­˜'æŒ‰é’®ã€‚")
            return None, False


# --- Streamlit åº”ç”¨ä¸»é€»è¾‘ ---

# 1. æ–‡ä»¶ä¸Šä¼ 
uploaded_files = st.sidebar.file_uploader(
    label="ä¸Šä¼  PDF æ–‡ä»¶",
    type=["pdf"],
    accept_multiple_files=True,
    key="pdf_uploader_dashscope_chroma_cn_v2"
)

# 2. é…ç½®æ£€ç´¢å™¨ (ä¼šæ£€æŸ¥æ–‡ä»¶å˜æ›´å¹¶å†³å®šåŠ è½½æˆ–åˆ›å»º)
retriever = None # åˆå§‹åŒ–ä¸º None
loaded_from_disk = False # åˆå§‹åŒ–ä¸º False
with st.spinner("æ­£åœ¨å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£æˆ–åŠ è½½ç°æœ‰çš„ Chroma æ£€ç´¢å™¨å¹¶æ£€æŸ¥æ–‡ä»¶å˜æ›´..."):
    retriever, loaded_from_disk = configure_retriever(uploaded_files)

# å¦‚æœæ£€ç´¢å™¨é…ç½®å¤±è´¥ï¼Œåœæ­¢åº”ç”¨ (æˆ–è€…ç»™å‡ºæç¤º)
if retriever is None:
    # å¦‚æœæ˜¯å› ä¸ºæ²¡æœ‰ä¸Šä¼ æ–‡ä»¶ä¸”æ²¡æœ‰æŒä¹…åŒ–æ•°æ®ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µï¼Œä¸éœ€è¦åœæ­¢
    if not uploaded_files and not os.path.exists(PERSIST_DIRECTORY):
        st.info("è¯·ä¸Šä¼  PDF æ–‡ä»¶ä»¥å¼€å§‹ä¸æ–‡æ¡£å¯¹è¯ã€‚")
        # å¯ä»¥é€‰æ‹©åœæ­¢ï¼Œæˆ–è€…è®©åº”ç”¨ç»§ç»­è¿è¡Œä½†ç¦ç”¨èŠå¤©è¾“å…¥
        # st.stop()
    else:
        # å¦‚æœæ˜¯å…¶ä»–é”™è¯¯ï¼ˆåŠ è½½å¤±è´¥ã€åˆ›å»ºå¤±è´¥ç­‰ï¼‰ï¼Œæ˜¾ç¤ºé”™è¯¯å¹¶åœæ­¢
        st.error("æœªèƒ½é…ç½®æˆ–åŠ è½½ Chroma æ–‡æ¡£æ£€ç´¢å™¨ï¼Œæ— æ³•ç»§ç»­ã€‚è¯·æ£€æŸ¥ä¸Šä¼ çš„æ–‡ä»¶ã€é”™è¯¯æ—¥å¿—æˆ–å°è¯•æ¸…é™¤ç¼“å­˜ã€‚")
        st.stop()

# 3. åˆå§‹åŒ– LLM
llm = None # åˆå§‹åŒ–
if retriever is not None: # ä»…åœ¨æ£€ç´¢å™¨æˆåŠŸé…ç½®åæ‰åˆå§‹åŒ– LLM
    try:
        st.info(f"æ­£åœ¨åˆå§‹åŒ– DashScope LLM: {MODEL_ID}...")
        llm = ChatOpenAI(
            model_name=MODEL_ID,
            openai_api_key=DASHSCOPE_API_KEY,
            openai_api_base=DASHSCOPE_API_BASE,
            temperature=0.5,
            max_tokens=1024,
        )
        st.success(f"DashScope LLM ({MODEL_ID}) åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        st.error(f"åˆå§‹åŒ– DashScope LLM ({MODEL_ID}) å¤±è´¥: {e}")
        logging.error("DashScope LLM åˆå§‹åŒ–å¤±è´¥:", exc_info=True)
        if "api_key" in str(e).lower() or "authenticate" in str(e).lower():
            st.error("è®¤è¯å¤±è´¥ã€‚è¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æ˜¯å¦å·²æ­£ç¡®è®¾ç½®ä¸” Key æœ¬èº«æœ‰æ•ˆã€‚")
        elif "base_url" in str(e).lower() or "connection" in str(e).lower():
             st.error(f"è¿æ¥å¤±è´¥ã€‚è¯·æ£€æŸ¥ Base URL ({DASHSCOPE_API_BASE}) æ˜¯å¦æ­£ç¡®ä»¥åŠç½‘ç»œè¿æ¥æ˜¯å¦èƒ½è®¿é—®è¯¥åœ°å€ã€‚")
        elif "model" in str(e).lower() or "not found" in str(e).lower():
             st.error(f"æ¨¡å‹é”™è¯¯ã€‚è¯·æ£€æŸ¥æ¨¡å‹åç§° '{MODEL_ID}' æ˜¯å¦æ˜¯ DashScope æ”¯æŒçš„æœ‰æ•ˆæ¨¡å‹ã€‚")
        else:
             st.error(f"è¯·æ£€æŸ¥ API Keyã€æ¨¡å‹åç§° ('{MODEL_ID}')ã€Base URL åŠç½‘ç»œè¿æ¥ã€‚")
        st.stop()

# 4. åˆå§‹åŒ–èŠå¤©è®°å½•
msgs = StreamlitChatMessageHistory(key="rag_chat_messages_dashscope_chroma_cn_v2_persistent")

# 5. å®šä¹‰ Prompt æ¨¡æ¿
RESPONSE_TEMPLATE = """<s>[INST]
<<SYS>>
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€è€å¿ƒã€ä¸”ä¹äºåŠ©äººçš„ AI åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥ä¾æ®ä¸Šä¸‹æ–‡è¿›è¡Œå›ç­”ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œæˆ–è€…é—®é¢˜ä¸ä¸Šä¸‹æ–‡æ— å…³ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œå¹¶å°è¯•æ ¹æ®ä½ çš„é€šç”¨çŸ¥è¯†è¿›è¡Œå›ç­”ï¼ˆå¦‚æœå¯èƒ½ï¼‰ï¼ŒåŒæ—¶è¯´æ˜è¿™æ˜¯åŸºäºé€šç”¨çŸ¥è¯†è€Œéæ–‡æ¡£å†…å®¹ã€‚
å›ç­”åº”æ¸…æ™°ã€ç®€æ´ï¼Œå¹¶ä½¿ç”¨ä¸­æ–‡ã€‚
<<SYS>>

ä»æ–‡æ¡£ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯:
---
{context}
---

ç”¨æˆ·é—®é¢˜: {question}
[/INST]
AI åŠ©æ‰‹å›ç­”:
"""
PROMPT = PromptTemplate(template=RESPONSE_TEMPLATE, input_variables=["context", "question"])

# 6. åˆ›å»º RAG é—®ç­”é“¾
qa_chain = None # åˆå§‹åŒ–
if llm is not None and retriever is not None: # ä»…åœ¨ LLM å’Œ Retriever éƒ½æˆåŠŸæ—¶åˆ›å»º
    try:
        st.info("æ­£åœ¨åˆ›å»º RAG é—®ç­”é“¾ (ä½¿ç”¨ ChromaDB å’Œ DashScope LLM)...")
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type='stuff',
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
        st.success("RAG é—®ç­”é“¾å‡†å¤‡å°±ç»ªã€‚")
    except Exception as e:
        st.error(f"åˆ›å»º RAG QA é“¾å¤±è´¥: {e}")
        logging.error("RAG é“¾åˆ›å»ºå¤±è´¥:", exc_info=True)
        st.stop()

# 7. æ˜¾ç¤ºèŠå¤©ç•Œé¢å’Œå¤„ç†ç”¨æˆ·è¾“å…¥

# ç¡®å®šåˆå§‹æ¶ˆæ¯
if len(msgs.messages) == 0 or st.sidebar.button("å¼€å§‹æ–°å¯¹è¯", key="new_chat_button_dashscope_chroma_cn_v2_persistent"):
    msgs.clear()
    initial_message = "ä½ å¥½ï¼è¯·ä¸Šä¼  PDF æ–‡æ¡£ï¼Œæˆ‘å¯ä»¥æ ¹æ®æ–‡æ¡£å†…å®¹å›ç­”ä½ çš„é—®é¢˜ã€‚"
    if retriever is not None: # å¦‚æœæ£€ç´¢å™¨å·²å°±ç»ª
        if loaded_from_disk:
            initial_message = f"å·²åŠ è½½ä¹‹å‰çš„æ–‡æ¡£æ•°æ®ï¼ˆæ¥è‡ª `{PERSIST_DIRECTORY}`ï¼‰ã€‚å…³äºè¿™äº›æ–‡æ¡£ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"
        elif uploaded_files:
            initial_message = f"å·²å¤„ç†æ‚¨ä¸Šä¼ çš„æ–‡æ¡£å¹¶åˆ›å»ºç´¢å¼•ã€‚å…³äºè¿™äº›æ–‡æ¡£ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"
    msgs.add_ai_message(initial_message)

# ä½¿ç”¨ Emojis ä½œä¸ºå¤´åƒ
avatars = {"human": "ğŸ§‘â€ğŸ’»", "ai": "ğŸ¤–"}

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for msg in msgs.messages:
    st.chat_message(msg.type, avatar=avatars.get(msg.type)).write(msg.content)

# å¤„ç†ç”¨æˆ·è¾“å…¥
# åªæœ‰åœ¨ qa_chain æˆåŠŸåˆ›å»ºåæ‰å¯ç”¨èŠå¤©è¾“å…¥æ¡†
chat_input_disabled = (qa_chain is None)
chat_input_placeholder = "è¯·å…ˆä¸Šä¼ æ–‡æ¡£å¹¶ç­‰å¾…å¤„ç†å®Œæˆ..." if chat_input_disabled else "å°±æ‚¨çš„æ–‡æ¡£æå‡ºé—®é¢˜..."

if user_query := st.chat_input(placeholder=chat_input_placeholder, key="user_query_input_dashscope_chroma_cn_v2_persistent", disabled=chat_input_disabled):
    msgs.add_user_message(user_query)
    st.chat_message("human", avatar=avatars["human"]).write(user_query)

    with st.chat_message("ai", avatar=avatars["ai"]):
        placeholder = st.empty()
        placeholder.markdown("æ€è€ƒä¸­ (æ­£åœ¨ä½¿ç”¨ ChromaDB æ£€ç´¢å’Œ DashScope LLM)...")
        try:
            # è°ƒç”¨ QA é“¾
            response = qa_chain.invoke({"query": user_query}) # ä½¿ç”¨ invoke
            answer = response.get("result")

            if answer is None:
                 answer = "æŠ±æ­‰ï¼Œæœªèƒ½ç”Ÿæˆå›ç­”ã€‚è¯·å°è¯•é‡æ–°æé—®æˆ–æ£€æŸ¥æ¨¡å‹çŠ¶æ€ã€‚"
                 logging.error(f"QA chain for query '{user_query}' returned None result. Response: {response}")

            placeholder.empty() # æ¸…é™¤ "æ€è€ƒä¸­"
            st.markdown(answer) # æ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆ
            msgs.add_ai_message(answer)

            # æ˜¾ç¤ºä¸Šä¸‹æ–‡æ¥æº (ä¿æŒç”¨äºè°ƒè¯•)
            retrieved_docs = response.get("source_documents", [])
            if retrieved_docs:
                with st.expander("æŸ¥çœ‹æœ¬æ¬¡æŸ¥è¯¢æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ¥æº", expanded=False):
                     for i, doc in enumerate(retrieved_docs):
                        source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
                        # å°è¯•æå–æ›´å¹²å‡€çš„æ–‡ä»¶å
                        source_display = os.path.basename(source) if source != 'æœªçŸ¥æ¥æº' else source
                        page = doc.metadata.get('page', '?')
                        if isinstance(page, int):
                             page += 1 # PDF é¡µç ä» 1 å¼€å§‹
                        st.write(f"**ä¸Šä¸‹æ–‡å— {i+1}:** (æ¥æº: {source_display}, é¡µç : {page})")
                        st.caption(doc.page_content[:800] + "..." if len(doc.page_content) > 800 else doc.page_content)
                        st.write("---")
            else:
                logging.warning("é—®ç­”é“¾æ²¡æœ‰è¿”å›æºæ–‡æ¡£ä¿¡æ¯ã€‚")

        except Exception as e:
            placeholder.empty()
            error_msg = f"å¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}"
            st.error(error_msg)
            logging.error(f"å¤„ç†æŸ¥è¯¢ '{user_query}' æ—¶å‡ºé”™:", exc_info=True)
            ai_error_message = f"æŠ±æ­‰ï¼Œå°è¯•å›ç­”æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†å†…éƒ¨é”™è¯¯ã€‚" # é»˜è®¤é”™è¯¯æ¶ˆæ¯
            if "api_key" in str(e).lower() or "authenticate" in str(e).lower():
                 ai_error_message = "æŠ±æ­‰ï¼Œå°è¯•å›ç­”æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº† API å¯†é’¥è®¤è¯é”™è¯¯ã€‚è¯·è”ç³»ç®¡ç†å‘˜æ£€æŸ¥é…ç½®ã€‚"
            elif "rate limit" in str(e).lower():
                 ai_error_message = "æŠ±æ­‰ï¼Œè¾¾åˆ°äº† API è°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œè¯·ç¨åå†è¯•ã€‚"
            elif "connection" in str(e).lower():
                  ai_error_message = "æŠ±æ­‰ï¼Œå°è¯•è¿æ¥ DashScope API æ—¶å‡ºé”™ï¼Œè¯·æ£€æŸ¥ç½‘ç»œã€‚"
            elif "context length" in str(e).lower():
                  ai_error_message = "æŠ±æ­‰ï¼Œæ‚¨çš„é—®é¢˜æˆ–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œè¶…å‡ºäº†æ¨¡å‹çš„å¤„ç†é™åˆ¶ã€‚è¯·å°è¯•ç¼©çŸ­é—®é¢˜æˆ–ä¸Šä¼ æ›´å°çš„æ–‡æ¡£ã€‚"
            # åœ¨èŠå¤©è®°å½•ä¸­æ·»åŠ æ›´å‹å¥½çš„é”™è¯¯æç¤º
            st.write(ai_error_message) # ä¹Ÿç›´æ¥æ˜¾ç¤ºç»™ç”¨æˆ·
            msgs.add_ai_message(ai_error_message)


# --- ä¾§è¾¹æ  "å…³äº" éƒ¨åˆ† ---
about = st.sidebar.expander("å…³äºæ­¤åº”ç”¨")
# åœ¨å…³äºéƒ¨åˆ†åŠ¨æ€æ˜¾ç¤ºæ˜¯å¦åŠ è½½äº†æŒä¹…åŒ–æ•°æ®
persistence_status = f'æŒä¹…åŒ–åˆ°ç›®å½• `{PERSIST_DIRECTORY}`' # æ˜¾ç¤ºç›¸å¯¹è·¯å¾„
# åªæœ‰åœ¨æ£€ç´¢å™¨æˆåŠŸé…ç½®åæ‰æ˜¾ç¤ºçŠ¶æ€
if retriever is not None:
    if loaded_from_disk:
         persistence_status += " (å·²åŠ è½½ç°æœ‰æ•°æ®)"
    elif uploaded_files:
         persistence_status += " (å·²æ ¹æ®ä¸Šä¼ æ–‡ä»¶åˆ›å»º/æ›´æ–°)"
    else: # retriever is not None but not loaded_from_disk and no uploaded files
         persistence_status += " (å·²å°±ç»ªï¼Œæ— æ–‡ä»¶ä¸Šä¼ )"
else:
     # å¦‚æœæ˜¯å› ä¸ºæ²¡ä¸Šä¼ æ–‡ä»¶ä¸”æ— æŒä¹…åŒ–æ•°æ®ï¼Œæ˜¾ç¤ºä¸åŒä¿¡æ¯
     if not uploaded_files and not os.path.exists(PERSIST_DIRECTORY):
         persistence_status = f"æŒä¹…åŒ–ç›®å½• `{PERSIST_DIRECTORY}` å°šæœªåˆ›å»º (ç­‰å¾…æ–‡ä»¶ä¸Šä¼ )"
     else:
         persistence_status = f"æŒä¹…åŒ–ç›®å½• `{PERSIST_DIRECTORY}` çŠ¶æ€æœªçŸ¥ (æ£€ç´¢å™¨é…ç½®å¤±è´¥)"


about.write(f"""
    è¿™æ˜¯ä¸€ä¸ªåŸºäº RAG (Retrieval-Augmented Generation) çš„ AI åŠ©æ‰‹ï¼Œå¯ä»¥ä¸æ‚¨ä¸Šä¼ çš„ PDF æ–‡æ¡£è¿›è¡Œå¯¹è¯ã€‚

    **å…³é”®ç‰¹æ€§:**
    *   æ”¯æŒä¸Šä¼  PDF æ–‡æ¡£ã€‚
    *   ä½¿ç”¨ BGE åµŒå…¥æ¨¡å‹å°†æ–‡æ¡£å†…å®¹è½¬æ¢ä¸ºå‘é‡ã€‚
    *   ä½¿ç”¨ ChromaDB å­˜å‚¨å’Œæ£€ç´¢æ–‡æ¡£å‘é‡ï¼ˆ**æ”¯æŒæŒä¹…åŒ–ä¸æ–‡ä»¶åŒæ­¥**ï¼‰ã€‚
    *   åˆ©ç”¨ DashScope çš„ {MODEL_ID} æ¨¡å‹æ ¹æ®æ£€ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚

    **æŠ€æœ¯æ ˆ:**
    *   **è¯­è¨€æ¨¡å‹:** {MODEL_ID} (é€šè¿‡é˜¿é‡Œäº‘ DashScope)
    *   **åµŒå…¥æ¨¡å‹:** BAAI BGE (langchain-huggingface, åœ¨ {'GPU' if torch.cuda.is_available() else 'CPU'} ä¸Šè¿è¡Œ)
    *   **æ£€ç´¢å™¨ & å‘é‡åº“:** ChromaDB ({persistence_status})
    *   **æ¡†æ¶:** Langchain & Streamlit

    **æ–‡ä»¶åŒæ­¥è¯´æ˜:**
    åº”ç”¨ä¼šè®¡ç®—ä¸Šä¼ æ–‡ä»¶çš„å†…å®¹å“ˆå¸Œå€¼ï¼Œå¹¶ä¸ä¹‹å‰ä¿å­˜çš„çŠ¶æ€ï¼ˆåœ¨ `{PERSIST_STATE_FILE}`ï¼‰è¿›è¡Œæ¯”å¯¹ã€‚å¦‚æœæ£€æµ‹åˆ°æ‚¨æœ¬æ¬¡ä¸Šä¼ çš„æ–‡ä»¶é›†åˆä¸ä¹‹å‰å¤„ç†çš„ä¸åŒï¼ˆåŒ…æ‹¬æ–‡ä»¶çš„æ·»åŠ ã€åˆ é™¤æˆ–å†…å®¹ä¿®æ”¹ï¼‰ï¼Œå°†è‡ªåŠ¨æ¸…é™¤æ—§æ•°æ®ï¼ˆåœ¨ `{PERSIST_DIRECTORY}` ç›®å½•ä¸‹ï¼‰ï¼Œå¹¶æ ¹æ®å½“å‰ä¸Šä¼ çš„æ‰€æœ‰æ–‡ä»¶é‡æ–°åˆ›å»ºå‘é‡æ•°æ®åº“ã€‚å¦‚æœä¸Šä¼ æ–‡ä»¶ä¸ä¹‹å‰ä¸€è‡´ï¼Œåˆ™å¿«é€ŸåŠ è½½å·²ä¿å­˜çš„æ•°æ®ã€‚

    **æ³¨æ„:** æŒä¹…åŒ–æ•°æ®ä¿å­˜åœ¨ `{PERSIST_DIRECTORY}` ç›®å½•ä¸­ã€‚è¯·ç¡®ä¿æ­¤ç›®å½•å·²è¢«æ·»åŠ åˆ° `.gitignore` æ–‡ä»¶ï¼Œä»¥é¿å…å°†å¤§é‡æ•°æ®æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶ã€‚
""")


